\documentclass[12pt,a4paper]{article}
\usepackage[a4paper, left=2.4cm, right=2.3cm, top=2.3cm, bottom=2.5cm]{geometry}
\hyphenpenalty=10000
\renewcommand{\tablename}{Tabela}
\usepackage{graphicx} % Required for inserting images
\usepackage{array}
\usepackage{caption}
\usepackage{microtype}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[polish]{babel}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{float}
\sloppy

\definecolor{darkgreen}{rgb}{0,0.70,0}
\definecolor{lightblue}{rgb}{0.0,0.42,0.91}
\definecolor{grey}{rgb}{0.60, 0.60, 0.60}

\title{Analiza wariancji}
\author{Weronika Przysiężna}
\date{Marzec 2025}

\setlength{\parindent}{0pt}
\lstset{
  language=Python,
  aboveskip=1em,
  basicstyle=\ttfamily\footnotesize,
  breaklines=true,
  abovecaptionskip=-6pt,
  captionpos=b,
  escapeinside={\%*}{*)},
  frame=single,
  numberstyle=\tiny,
  keywordstyle=\color{lightblue},
  commentstyle=\color{grey},
  stringstyle=\color{darkgreen},
  showstringspaces=false,
  tabsize=4,
  literate={{ą}{{\k a}}1
           {Ą}{{\k A}}1
           {ż}{{\. z}}1
           {Ż}{{\. Z}}1
           {ź}{{\' z}}1
           {Ź}{{\' Z}}1
           {ć}{{\' c}}1
           {Ć}{{\' C}}1
           {ę}{{\k e}}1
           {Ę}{{\k E}}1
           {ó}{{\' o}}1
           {Ó}{{\' O}}1
           {ń}{{\' n}}1
           {Ń}{{\' N}}1
           {ś}{{\' s}}1
           {Ś}{{\' S}}1
           {ł}{{\l}}1
           {Ł}{{\L}}1}
}

\begin{document}

\maketitle
\newpage
\section{Teoretyczne wprowadzenie do jednoczynnikowej analizy wariancji}

\subsection{Wstęp}
Analiza wariancji jest analizą statystyczną, która wykrywa różnice między dwiema lub więcej grupami określonymi dla pojedynczego czynnika lub zmiennej niezależnej. Identyfikuje ona zmienność lub wariancję pomiędzy obserwacjami przypisując ją różnym źródłom, które (po odpowiednim przetestowaniu) wskazują, czy zaobserwowane różnice między średnimi grupowymi są prawdopobnie rzeczywiste, czy jedynie wynikiem przypadku. \cite{witte2010statistics}\\
\\W naszym badaniu skupimy się na wykrywaniu różnic przy pomiarach na różnych osobach.

\subsection{Założenia dotyczące jednoczynnikowej analizy wariancji (ANOVA) z jednym czynnikiem międzyobiektowym \cite{orourke2005sas}}

\begin{itemize}
    \item \textbf{Zmienna zależna mierzona na skali ilościowej}: zmienna zależna powinna być zmienną ilościową (na poziom interwałowym lub ilorazowym).
    \item \textbf{Losowość i niezależność obserwacji}: nie ma związku między obserwacjami w każdej grupie lub między samymi grupami, a w każdej grupie są różni uczestnicy badania i żaden uczestnik nie należy do więcej niż jednej grupy;  uczestnicy badania są dobierani losowo.
    \item \textbf{Równoliczność obserwacji w grupach}: poszczególne kategorie zmiennej niezależnej powinny być statystycznie równoliczne (aby sprawdzić, czy analizowane grupy różnią się istotnie statycznie pod względem liczebności, można zastosować test zgodności Chi-kwadrat).
    \item \textbf{Rozkład normalny}: rozkład wyników w analizowanych grupach jest zbliżony do rozkładu normalnego (oceny tego założenia można dokonać stosując test Kołomogorowa-Smirnova lub Shapiro-Wilka).
    \item \textbf{Wariancje w grupach są jednorodne (homogeniczność wariancji)}: zmienność w każdej porównywanej grupie powinna być podobna; jeśli wariancje różnią się między grupami, to można zastosować test Welcha lub Browna-Forsythe'a, które wprowadzają poprawkę na nierówne wariancje do statystyki F.

\end{itemize}

\newpage
\subsection{Jednoczynnikowa ANOVA}

\subsubsection{Suma kwadratów SS (ang. Sum of Squares)}
Wariancja próby mierzy zmienność w dowolnym zbiorze obserwacji poprzez obliczenie sumy kwadratów odchyleń od ich średniej:
$$SS=\sum(X-\overline{X})^2.$$
Następnie suma kwadratów $SS$ jest dzielona przez liczbę stopni swobody $n-1$:
$$s^2=\frac{SS}{df},$$
gdzie:
\begin{itemize}
    \item $\overline{X}$ - średnia próby,
    \item $s^2$ - wariancja próby,
    \item $df=n-1$ - stopnie swobody.
\end{itemize}

\subsubsection{Średnia kwadratów MS (ang. Mean Square)}
Średnia kwadratów to oszacowanie wariancji uzyskane przez podzielenie sumy kwadratów $SS$ przez liczbę stopni swobody $n-1$.
Ogólny wzór na oszacowanie wariancji ma postać:
$$MS=\frac{SS}{df},$$
gdzie:
\begin{itemize}
    \item $MS$ - średnia kwadratów,
    \item $SS$ - suma kwadratów odchyleń od średniej,
    \item $df = n-1$ - liczba stopni swobody.
\end{itemize}

\newpage
\subsubsection{Wzory definicyjne na sumy kwadratów \cite{witte2010statistics}:}
\begin{enumerate}
    \item $SS_{total}$ - całkowita suma kwadratów odchyleń od średniej ogólnej (zmienność całkowita)
    $$SS_{total}=\sum(X-\overline{X}_{grand})^2.$$
    Równoważny wzór obliczeniowy:
    $$SS_{total}=\sum X^2-\frac{G^2}{N}.$$
    \item $SS_{between}$ - suma kwadratów odchyleń średnich grupowych od średniej ogólnej (zmienność między grupami)
    $$SS_{between}=\sum n(\overline{X}_{group}-\overline{X}_{grand})^2.$$
    Równoważny wzór obliczeniowy:
    $$SS_{between}=\sum \frac{T^2}{n}-\frac{G^2}{N}.$$
    \item $SS_{within}$ - suma kwadratów odchyleń indywidualnych wyników w grupie od średnich grupowych (zmienność wewnątrz grup)
    $$SS_{within}=\sum(X-\overline{X}_{group})^2.$$
    Równoważny wzór obliczeniowy:
    $$SS_{within}=\sum X^2-\sum\frac{T^2}{n}.$$
    \item Sprawdzamy dokładność obliczeniową weryfikując równość:
    $$SS_{total}=SS_{between}+SS_{within}$$
\end{enumerate}
Oznaczenia:
\begin{itemize}
    \item $X$ - pojedyncza wartość obserwowana,
    \item $\overline{X}_{group}$ - średnia dla danej grupy,
    \item $\overline{X}_{grand}$ - średnia ogólna dla całej próby,
    \item $T$ - suma wartości w grupie,
    \item $n$ - liczba obserwacji w grupie,
    \item $G$ - suma wartości dla wszystkich grup (suma ogólna),
    \item $N$ - całkowita liczba obserwacji (sumaryczna wielkość próby). 
\end{itemize}

\newpage
\subsubsection{Stopnie swobody ($df$):}
\begin{enumerate}
    \item $df_{total}=N-1$,
    \item $df_{between}=k-1$,
    \item $df_{within}=N-k$,
\end{enumerate}
gdzie:
\begin{itemize}
    \item $N$ - całkowita liczba obserwacji (sumaryczna wielkość próby),
    \item $k$ - liczba grup.
\end{itemize}
\vspace{2mm}
Sprawdzamy dokładność obliczeń weryfikując równość:
$$df_{total}=df_{between}+df_{within}.$$

\subsubsection{Wzory na średnie kwadratów \cite{witte2010statistics}:}
\begin{enumerate}
    \item $MS_{between}$ - średni kwadrat odchyleń między grupami (zmienność między średnimi dla grup)
    $$MS_{between}=\frac{SS_{between}}{df_{between}}$$
    \item $MS_{within}$ - średni kwadrat odchyleń wewnątrz grupy (zmienność wyników wewnątrz grupy; mierzy jedynie błąd losowy)
    $$MS_{within}=\frac{SS_{within}}{df_{within}}=MS_{error}$$
\end{enumerate}

\newpage
\subsubsection{Rozkład F-Snedecora}
Liczymy statystykę testową $F$ jako:
$$F=\frac{MS_{between}}{MS_{within}}$$
\\
Określamy obszar krytyczny jako:
$$Q=\{F:F\geq F_{\alpha}\},$$
gdzie $F_{\alpha}$ jest wartością krytyczną odczytaną z tablic rozkładu F-Snedecora dla $(df_{between},df_{within})$ stopni swobody, czyli  $F(df_{between},df_{within})$.
\\
\begin{enumerate}
    \item Jeżeli $F\in Q$ ($F\geq F_{\alpha}$), to odrzucamy hipotezę zerową $H_0$ na korzyść hipotezy alternatywnej $H_A$ i wnioskujemy, że badane średnie nie są sobie równe.
    \item Jeżeli $F\not\in Q$ ($F< F_{\alpha}$), to nie ma podstaw do odrzucenia hipotezy zerowej $H_0$ i wnioskujemy, że badane średnie są równe.
\end{enumerate}

\subsubsection{Testy post-hoc (test HSD Tukey'a)}
Odrzucenie hipotezy zerowej w analizie ANOVA pozwala nam stwierdzić, że istnieją różnice między średnimi
w badanych grupach, nie mówi nam jednak, które z badanych średnich są różne.
Testy post-hoc, znane także jako porównania wielokrotne lub parami, są stosowane po analizie wariancji w celu określenia różnic pomiędzy średnimi.
\\

Test HSD Tukeya (ang. Tukey’s honest significant difference test) bazuje na rozkładzie “studentized range
(q) distribution” - rozkładzie zbliżonym do rozkładu t-studenta. Dla każdej hipotezy zerowej: $$H_0 : \mu_i = \mu_j$$
statystyka testowa dana jest wzorem:
$$HSD = q \sqrt{\frac{MS_{error}}{n}},$$
gdzie:
\begin{itemize}
    \item $HSD$ - minimalna różnica między dwiema średnimi, która jest uznawana za istotną statystycznie,
    \item $q$ - wartość krytyczna, którą odczytujemy z tabeli wartości krytycznych dla testu HSD Tukey'a; zależy od liczby poziomów zmiennej niezależnej i liczby stopni swobody błędu,
    \item $MS_{error}$ - (Mean Square Error) wariancja błędu z analizy ANOVA - miara tego, jak bardzo wartości różnią się od siebie w obrębie każdej grupy,
    \item $n$ - liczba osób (próbek) w każdej grupie (zakładamy równoliczne grupy; jeśli grupy są nierównoliczne, stosuje się uśrednioną wielkość próby).
\end{itemize}

\newpage
\subsubsection{Obliczenie siły efektu (d Cohen'a)}
Wielkość efektu dla dowolnej istotnej różnicy między parami średnich można oszacować za pomocą współczynnika d Cohena dla testu ANOVA dla niezależnych grup (czyli każda osoba jest w innej grupie):
$$d=\frac{\overline{X}_1-\overline{X}_2}{s^2_p},$$
gdzie:
\begin{itemize}
    \item $\overline{X}_1-\overline{X}_2$ - różnica między średnimi dwóch grup,
    \item $s_p$ - wspólne odchylenie standardowe liczone jako: $$s_p=\sqrt{\frac{(n_1-1)s^2_1+(n_2-1)s^2_2}{n_1+n_2-2}},$$ gdzie:
    \begin{itemize}
        \item $s^2_1$, $s^2_2$ - wariancje w grupach 1 i 2,
        \item $n_1$, $n_2$ - liczności próbek w grupach 1 i 2,
        \item $n_1+n_2-2$ - stopnie swobody dla wariancji błędu,
    \end{itemize}
\end{itemize}
czyli uśredniamy wariancję w obu grupach.

\subsubsection{Podsumowanie}
Jednoczynnikowa analiza wariancji (ANOVA) jest metodą statystyczną służącą do wykrywania istotnych różnic między średnimi grup, opartą na porównaniu wariancji. W ramach tej analizy zakłada się, że zmienne zależne są mierzone na skali ilościowej, a obserwacje są losowe i niezależne, z równolicznością grup i jednorodnością wariancji. \\

Obliczenia obejmują sumy kwadratów (SS) oraz średnie kwadraty (MS), które są podstawą do obliczenia statystyki F-Snedecora. W przypadku odrzucenia hipotezy zerowej, stosuje się testy post-hoc, takie jak test Tukeya, które pozwalają na identyfikację par grup różniących się istotnie. Dodatkowo, siła efektu (d Cohena) jest wykorzystywana do oceny praktycznej istotności zaobserwowanych różnic. \\

W kolejnej części pracy przedstawione zostaną wyniki praktyczne analizy.

% ----------------------------------------------------------------------------------------------------------------



\newpage
\section{Praktyczne zastosowanie analizy wariancji}
W celu zilustrowania praktycznego zastosowania analizy wariancji przeprowadziłam badania na dwóch zestawach danych:

\begin{enumerate}
    \item Dane uzyskane za pośrednictwem ankiet, których celem jest analiza hipotetycznych zależności między czasem poświęcanym tygodniowo na aktywność fizyczną a nawykami oraz jakością życia.
    \item Dane udostępnione na platformie Kaggle.com, umożliwiające zbadanie potencjalnych zależności między [...] \footnote{Dane wykorzystane w analizie pochodzą z platformy Kaggle.com i zostały syntetycznie wygenerowane. Oznacza to, że nie są wynikiem rzeczywistych pomiarów, lecz zostały stworzone na podstawie określonych założeń i symulacji.}
\end{enumerate}
W dalszej części omówię proces pozyskania obu zbiorów danych oraz przedstawię w jaki sposób przeprowadziłam analizę statystyczną danych.


\subsection{Badanie zależności między czasem poświęcanym na aktywność fizyczną a jakością życia}


\subsubsection{Wymagania dotyczące danych}
Aby dane mogły zostać wykorzystane w analizie wariancji, konieczne było oszacowanie minimalnej liczebności prób potrzebnej do uzyskania istotnych statystycznie wyników. W tym celu przygotowano skrypt w języku Python, wykorzystujący funkcję \texttt{solve\_power} z biblioteki \texttt{statsmodels.stats.power}. Poniższy kod wyznacza minimalną liczebność każdej grupy, niezbędną do przeprowadzenia testu ANOVA z zakładaną mocą:
\begin{lstlisting}
from statsmodels.stats.power import FTestAnovaPower
from math import ceil

# parametry
alpha = 0.05  # poziom istotności
power = 0.8   # moc testu
groups = 3    # liczba grup
fCohen = 0.25 # wskaźnik siły efektu f Cohena
              # niski = 0.10, średni = 0.25, wysoki = 0.40

def group_size(alpha, power, groups, fCohen):
    return ceil(FTestAnovaPower().solve_power(effect_size=fCohen, k_groups=groups, alpha=alpha, power=power))

print("Alfa:", alpha, "\nMoc testu:", power,
      "\nLiczba grup:", groups, "\nWskaźnik f Cohena:", fCohen,
      "\nLiczebność każdej grupy:", 
            group_size(alpha, power, groups, fCohen),
      "\nŁączna liczba badanych:", 
            groups * group_size(alpha, power, groups, fCohen))
\end{lstlisting}
W wyniku przeprowadzonych obliczeń uzyskano minimalną liczebność $n=158$ osób w każdej grupie, co daje łączną liczbę $N=474$ uczestników niezbędnych do przeprowadzenia analizy wariancji z założonymi parametrami:
\begin{lstlisting}
Alfa: 0.05 
Moc testu: 0.8 
Liczba grup: 3 
Wskaźnik f Cohena: 0.25 
Liczebność każdej grupy: 158 
Łączna liczba badanych: 474
\end{lstlisting}

\subsubsection{Metodologia pozyskiwania danych ankietowych}
Dane zostały zebrane za pomocą ankiety utworzonej w Google Forms i udostępnionej w mediach społecznościowych, w tym na grupach na Facebooku zrzeszających osoby aktywne fizycznie oraz studentów, a także na prywatnych profilach na Facebooku i Instagramie, skierowanych do znajomych.


Grupą badaną są młodzi dorośli w wieku 18-35 lat, posługujący się językiem polskim, posiadający dostęp do internetu oraz do mediów społecznościowych.

Pytaliśmy uczestników badania o ich wiek, płeć oraz odpowiedzi na poniższe pytania:
\begin{enumerate}
    \item {Ile czasu średnio tygodniowo poświęcasz na aktywność fizyczną? \\ (np. siłownia/rower/bieganie/taniec/joga)}
    \item Ile czasu średnio tygodniowo spędzasz na pozasportowych spotkaniach towarzyskich? Chodzi o spotkania ze znajomymi poza szkołą/miejscem pracy.
    \item {Ile czasu średnio dziennie spędzasz przed ekranem poza pracą/szkołą? \\ 
    (TV, komputer, telefon)}
    \item Ile czasu średnio śpisz w ciągu doby?
    \item {Ile razy chorowałeś/chorowałaś w ciągu ostatnich 12 miesięcy? \\
    (przeziębienie, grypa, choroby inne niż przewlekłe)}
    \item Na ile (w skali 1 - 10) oceniasz swoje ogóle poczucie szczęścia i zadowolenia z życia?
\end{enumerate}
\vspace{4mm}

Możliwe do wyboru odpowiedzi dla poszczególnych pytań prezentuje tabela:
\begin{table}[h]
    \centering
    \begin{tabular}{|l|l|l|}
        \hline \textbf{Pytania 1, 2, 3, 4} & \textbf{Pytanie 5} & \textbf{Pytanie 6} \\ \hline 
        mniej niż 1 godzinę & 0 - wcale &\\ \hline
        1 - 2 godziny & 1 raz & 1 - bardzo źle \\ \hline
        2 - 3 godziny & 2 razy & 2 \\ \hline
        3 - 4 godziny & 3 razy & 3 \\ \hline
        4 - 5 godzin & 4 razy & 4 \\ \hline
        5 - 6 godzin & 5 razy & 5 \\ \hline
        6 - 7 godzin & 6 razy & 6 \\ \hline
        7 - 8 godzin & 7 razy & 7 \\ \hline
        8 - 9 godzin & 8 razy & 8 \\ \hline
        więcej niż 9 godzin & 9 razy & 9 \\ \hline
         & 10 - 10 lub więcej niż 10 razy & 10 - bardzo dobrze \\ \hline
    \end{tabular}
    \caption{Odpowiedzi do wyboru.}
\end{table}

\newpage
\subsubsection{Wyniki ankiety}
W wyniku ankiety uzyskaliśmy 597 odpowiedzi.

\vspace{2mm}
Uzyskane dane wymagały przetworzenia w formę umożliwiającą ich dalszą analizę, tak aby odpowiedzi na pytania 1-6 były zapisane jako dane numeryczne oraz aby wiek uczestników nie przekraczał 35 lat. W tym celu opracowałam program w języku Python, który automatycznie wykonuje te operacje:

\vspace{3mm}
\begin{lstlisting}
import pandas as pd

dane = pd.read_excel('dane.xlsx')       # wczytanie pliku

dane = dane.iloc[:, 1:]                 # usunięcie 1. kolumny

dane.columns = ['wiek', 'płeć', 
                'akt_fiz', 'spot_tow',
                'ekran', 'sen', 
                'choroby', 'szczęście'] # zmiana nazw kolumn

dane = dane[dane['wiek'] <= 35]         # usunięcie wierszy, 
                                        # w których wiek > 35

dane = dane.reset_index(drop=True)      # resetowanie indeksów

# funkcja do konwersji wartosci w kolumnach
def konwertuj(wartosc):
    if 'mniej niż' in wartosc:
        return 0
    elif 'więcej niż' in wartosc:
        return 9
    elif '-' in wartosc:
        return int(wartosc.split('-')[0].strip())
    elif 'Kobieta' in wartosc:
        return 'K'
    elif 'Mężczyzna' in wartosc:
        return 'M'
    elif 'Inne' in wartosc:
        return 'I'

dane['płeć'] = dane['płeć'].astype(str).apply(konwertuj)
dane['akt_fiz'] = dane['akt_fiz'].astype(str).apply(konwertuj)
dane['spot_tow'] = dane['spot_tow'].astype(str).apply(konwertuj)
dane['ekran'] = dane['ekran'].astype(str).apply(konwertuj)
dane['sen'] = dane['sen'].astype(str).apply(konwertuj)

print(dane)
\end{lstlisting}

\newpage W rezultacie pozostały 522 wiersze z odpowiedziami.

\begin{lstlisting}
      wiek płeć  akt_fiz  spot_tow  ekran  sen  choroby  szczęście
0      31    M        9         7      4    6        2          7
1      25    M        2         3      2    6        5          7
2      29    M        2         5      5    7        4          3
3      25    M        6         0      9    6        3          4
4      29    M        3         2      7    7        1         10
..    ...  ...      ...       ...    ...  ...      ...        ...
517    25    M        6         7      3    8        1          9
518    32    M        0         3      3    5        1          7
519    32    M        1         9      1    7        5          8
520    25    M        0         2      3    8        1          5
521    25    M        0         0      9    6        2          4
\end{lstlisting}

\subsubsection{Wstępna analiza zebranych danych}

Wywołanie \texttt{describe} dla wybranych kolumn pozwoliło na uzyskanie zbiorczego opisu statystycznego tych zmiennych. 
\begin{lstlisting}
    print(dane[['wiek', 'płeć', 'akt_fiz', 'spot_tow', 'ekran', 
    'sen', 'choroby', 'szczęście']].describe(include='all'))
\end{lstlisting}
W wyniku wywołania powyższej linijki kodu otrzymujemy:
\begin{lstlisting}[basicstyle=\ttfamily\fontsize{7}{8}\selectfont\color{black},
  keywordstyle=\color{black},
  commentstyle=\color{black},
  stringstyle=\color{black},
  frame=single]
              wiek płeć     akt_fiz    spot_tow       ekran         sen     choroby   szczęście
count   522.000000  522  522.000000  522.000000  522.000000  522.000000  522.000000  522.000000
unique         NaN    3         NaN         NaN         NaN         NaN         NaN         NaN
top            NaN    M         NaN         NaN         NaN         NaN         NaN         NaN
freq           NaN  373         NaN         NaN         NaN         NaN         NaN         NaN
mean     26.168582  NaN    3.340996    3.105364    3.894636    6.358238    2.055556    6.281609
std       3.857631  NaN    2.825475    2.753189    2.415245    1.185331    1.784659    2.155920
min      18.000000  NaN    0.000000    0.000000    0.000000    0.000000    0.000000    1.000000
25%      24.000000  NaN    1.000000    1.000000    2.000000    6.000000    1.000000    5.000000
50%      26.000000  NaN    3.000000    2.000000    3.000000    6.000000    2.000000    7.000000
75%      29.000000  NaN    5.000000    5.000000    5.000000    7.000000    3.000000    8.000000
max      35.000000  NaN    9.000000    9.000000    9.000000    9.000000   10.000000   10.000000
\end{lstlisting}

Funkcja zwróciła m.in.:
\begin{itemize}
    \item liczność (\texttt{count}) - liczbę dostępnych wartości,
    \item średnią (\texttt{mean}) - wartość przeciętną,
    \item odchylenie standardowe (\texttt{std}) - miarę rozproszenia danych,
    \item minimalną i maksymalną wartość (\texttt{min}, \texttt{max}),
    \item percentyle (\texttt{25\%}, \texttt{50\%} (mediana), \texttt{75\%}), które pokazują, jak dane są rozłożone.
    \item liczność (\texttt{count}),
    \item liczba unikalnych wartości (\texttt{unique}),
    \item najczęściej występująca wartość (\texttt{top}),
    \item częstość występowania tej wartości (\texttt{freq}).
\end{itemize}

Dzięki tym informacjom możliwe było uzyskanie pierwszego wglądu w rozkład danych.

\newpage
\subsubsection{Podział badanych na grupy względem aktywności fizycznej}
Kolejnym etapem analizy był podział uczestników na grupy w zależności od deklarowanej liczby godzin poświęcanych tygodniowo na aktywność fizyczną. Grupy zostały zdefiniowane tak, aby ich liczebności były możliwie zbliżone, a przedziały czasowe dobrano empirycznie na podstawie rozkładu zmiennej \texttt{akt\_fiz}. W tym celu opracowano skrypt w języku Python, który tworzy nową kolumnę \texttt{'grupa'}, przypisując jej wartości \texttt{1}, \texttt{2} lub \texttt{3}:
\begin{lstlisting}
import pandas as pd

# definiowanie przedziałów
bins = [-1, 1, 4, 11]   # przedziały: 0-1h, 2-4h, >5h
labels = [1, 2, 3]      # oznaczenia grup

# przypisanie grup do nowej kolumny na podstawie przedziałów
dane['grupa'] = pd.cut(dane['akt_fiz'], bins=bins,
                labels=labels, right=True) 
            
# zliczanie wartości w kolumnie 'grupa'
print(dane['grupa'].value_counts()) 
\end{lstlisting}

Otrzymane wyniki wskazują, że grupy charakteryzują się zbliżoną liczebnością:
\begin{lstlisting}
grupa
2    197
1    169
3    156
\end{lstlisting}

\subsubsection{Test równoliczności grup}

Aby upewnić się, że liczebności poszczególnych grup są wystarczająco zbliżone, przeprowadzono test chi-kwadrat dopasowania. Test ten pozwala ocenić, czy zaobserwowany rozkład liczebności w grupach istotnie różni się od rozkładu równomiernego.

Do analizy wykorzystano funkcję \texttt{chisquare} z biblioteki \texttt{scipy.stats}:

\begin{lstlisting}
import pandas as pd
from scipy.stats import chisquare

alpha = 0.05        # poziom istotności
liczebnosci = dane['grupa'].value_counts().sort_index()

# test chi-kwadrat dla liczebności grup
stat, p_value = chisquare(liczebnosci)

print(f"Statystyka testu chi-kwadrat: {stat}")
print(f"p-wartość: {p_value}")

if p_value < alpha:
    print("Grupy NIE są równoliczne na poziomie istotności 0.05.")
else:
    print("Grupy można uznać za równoliczne.") # interpretacja
\end{lstlisting}
\newpage
Wynik:
\begin{lstlisting}
Statystyka testu chi-kwadrat: 5.045977011494253
p-wartość: 0.08021951169377096
Grupy można uznać za równoliczne.
\end{lstlisting}
Na podstawie testu chi-kwadrat ($\chi^2\approx5.046$) wyznaczona wartość  $p\approx0.08$ jest większa od $\alpha=0.05$. Wnioskujemy zatem, że nie stwierdzono istotnych statystycznie różnic w liczebnościach między grupami na poziomie istotności, co oznacza, że grupy można uznać za równoliczne.
\\

\textbf{Obliczenie $\chi^2$}
\\

Test $\chi^2$ dobroci dopasowania służy do oceny zgodności pomiędzy wartościami zaobserwowanymi dla $k$ kategorii jednej cechy $X$, a wartościami oczekiwanymi, wynikającymi z przyjętego rozkładu teoretycznego.
\\
Wzór na statystykę testową $\chi^2$:
$$\chi^2 = \sum_{i=1}^{k} \frac{(O_i - E_i)^2}{E_i},$$
gdzie
\begin{itemize}
    \item $O_i$ - liczebność zaobserwowana dla $X_i$,
    \item $E_i$ - liczebność oczekiwana dla $X_i$ (średnia arytmetyczna, tzn. $E_i=\frac{1}{k}\sum_{j=1}^{k}E_j$).
\end{itemize}
Statystyka ta ma  rozkład chi-kwadrat z liczbą stopni swobody wyznaczaną według wzoru $$df=k-1.$$

Hipotezy statystyczne w teście $\chi^2$ formułuje się następująco:

\begin{itemize}
    \item[$H_0$:] $O_i = E_i$ dla wszystkich kategorii (brak różnic między rozkładami),
    \item[$H_1$:] $O_i \neq E_i$ dla przynajmniej jednej kategorii (istnieje istotna różnica).
\end{itemize}

Test ten zastosujemy do sprawdzenia, czy grupy są równoliczne.
\\
Nasze dane:
\begin{itemize}
    \item $k=3 \implies df=k-1=2$,
    \item $O_1=169$, $O_2= 197$, $O_3=156$.
\end{itemize}
Obliczamy liczebność oczekiwaną $E_i$ ze wzoru $$E_i=\frac{1}{k}\sum_{j=1}^{k}E_j=\frac{169+197+156}{3}=\frac{522}{3}=174.$$

Następnie wyznaczamy wartość statystyki testowej $\chi^2$:
$$\chi^2 = \sum_{i=1}^{k} \frac{(O_i - E_i)^2}{E_i}= \frac{(169-174)^2+(197-144)^2+(156-174)^2}{174}= \frac{878}{174}\approx5.046.$$

Otrzymana na drodze obliczeń wartość $\chi^2$ jest zgodna z wartością otrzymaną w programie Python.


\\

\begin{table}[H]
\caption{Wartości krytyczne rozkładu $\chi^2$ dla $df = 2$ i wybranych poziomów istotności}
\begin{tabular}{|c|ccccccccccc|}
\hline
\textbf{df} \textbackslash \textbf{p} & 0.99 & 0.95 & 0.90 & 0.80 & 0.50 & 0.30 & 0.20 & \textbf{0.10} & \textbf{0.05} & 0.02 & 0.01 \\
\hline
2 & 0.020 & 0.103 & 0.211 & 0.446 & 1.386 & 2.408 & 3.219 & \textbf{4.605} & \textbf{5.991} & 7.824 & 9.210 \\
\hline
\end{tabular}
\end{table}
\\
Dla obliczonej wartości $\chi^2\approx0.0546$ i stopni swobody $df=2$ wyznaczamy wartość p. Z powyższej tabeli odczytujemy, że $0.05<p<0.10$, zatem $p>\alpha=0.05.$ \\Oznacza to brak podstaw do odrzucenia hipotezy $H_0$ i pozwala uznać grupy za równoliczne.

\subsubsection{Test normalności rozkładu}

W celu sprawdzenia, czy dane pochodzą z rozkładu normalnego, wykorzystuje się testy statystyczne oceniające stopień zgodności rozkładu empirycznego z teoretycznym rozkładem normalnym. Poniżej przedstawiono trzy najczęściej stosowane testy normalności.
Sprawdzenie normalności danych jest istotnym krokiem w analizie statystycznej. Poniżej przedstawiono cztery popularne testy służące do tego celu.

\paragraph{Test Kołmogorowa-Smirnowa (K-S)}
Test ten porównuje dystrybuantę empiryczną $F_n(x)$ z dystrybuantą teoretyczną $F(x)$ rozkładu normalnego. Statystyka testowa:

$$
D = \sup_x |F_n(x) - F(x)|
$$

 Jeśli $D$ jest większe niż wartość krytyczna, odrzucamy hipotezę zerową o normalności rozkładu.
Używamy tego testu, gdy parametry rozkładu (średnia i odchylenie standardowe) są znane.

\paragraph{Test Andersona-Darlinga}

Wariacja testu K-S, bardziej czuła na końce rozkładu. Statystyka testowa:

$$
A^2 = -n - \frac{1}{n} \sum_{i=1}^{n} (2i - 1) \left[\ln F(X_{(i)}) + \ln(1 - F(X_{(n+1-i)}))\right]
$$

Im większe $A^2$, tym większe odchylenie od normalności. Porównujemy z wartościami krytycznymi.
Zalecany do małych i średnich prób, bardzo czuły na odstępstwa w ogonach.

\paragraph{Test Shapiro-Wilka}

Skonstruowany specjalnie do testowania normalności w małych próbach. Statystyka testowa:

$$
W = \frac{\left( \sum_{i=1}^{n} a_i X_{(i)} \right)^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2}
$$

gdzie $a_i$ są wagami zależnymi od kowariancji i średnich wartości statystyk porządkowych z rozkładu normalnego.
Jeśli $W$ znacznie odbiega od 1, odrzucamy hipotezę normalności.
Najlepszy wybór przy małych próbach ($n < 50$), ale działa dobrze nawet do 2000 obserwacji.

\paragraph{Test Lillieforsa}

Modyfikacja testu Kołmogorowa-Smirnowa dla przypadku, gdy parametry rozkładu są szacowane z próby. Statystyka testowa:

$$
D^* = \sup_x |F_n(x) - F_{\hat{\mu}, \hat{\sigma}}(x)|
$$

Interpretacja jest podobna do testu K-S, ale używane są inne wartości krytyczne (ze względu na estymację parametrów). Używany, gdy nie znamy rozkładu a priori i musimy oszacować średnią i odchylenie.




\begin{table}[h!]
\centering
\footnotesize
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Test} & \textbf{Parametry} & \textbf{Czułość} & \textbf{Rozmiar próby} & \textbf{Zalecenia} \\ 
\hline
Kołmogorow-Smirnow & Znane & Umiarkowana & Dowolny & Znamy parametry \\
Anderson-Darling & Nieznane & Wysoka & Małe/średnie & Precyzyjny dla ogonów \\
Shapiro-Wilk & Nieznane & Bardzo wysoka & Małe do 2000 & Najlepszy ogólny wybór \\
Lilliefors & Nieznane & Umiarkowana & Małe/średnie & Nie znamy parametrów \\ 
\hline
\end{tabular}
\caption{Porównanie wybranych testów normalności}
\end{table}
\\ 
\paragraph{Implementacja testów normalności}
Ze względu na dużą liczebność badanych grup, testy normalności dla mojego zbioru danych zostały przeprowadzone wyłącznie z wykorzystaniem kodu w języku Python, co umożliwia automatyczne i dokładne wyznaczenie statystyk testowych oraz wartości p.

\\\vspace{5mm}

Przeprowadzono testy normalności dla każdej z pięciu analizowanych zmiennych: \texttt{spot\_tow}, \texttt{ekran}, \texttt{sen}, \texttt{choroby} oraz \texttt{szczęście}. Dla każdej zmiennej wykonano osobno testy: Kołmogorowa-Smirnowa, Andersona-Darlinga, Shapiro-Wilka oraz Lillieforsa, osobno dla każdej z trzech grup badanych.

\vspace{2mm}

Poniżej znajduje się fragment kodu odpowiadający za przeprowadzenie testów normalności. W ramach działania kodu obliczane są odpowiednie statystyki testowe oraz wartości p, które umożliwiają weryfikację hipotezy zerowej o zgodności rozkładu empirycznego z rozkładem normalnym. Dla testu Andersona-Darlinga porównywana jest również wartość statystyki z odpowiednią wartością krytyczną, zależną od poziomu istotności.
\begin{lstlisting}
import pandas as pd
from scipy import  stats
from statsmodels.stats.diagnostic import lilliefors

zmienna = 'spot_tow'      # analizowana zmienna
# zmienna = 'ekran'
# zmienna = 'sen'
# zmienna = 'choroby'
# zmienna = 'szczęście'

# podział na grupy
grupa_1 = dane[dane['grupa'] == 1][zmienna]
grupa_2 = dane[dane['grupa'] == 2][zmienna]
grupa_3 = dane[dane['grupa'] == 3][zmienna]

grupa_badana = grupa_1    # analizowana grupa
# grupa_badana = grupa_2
# grupa_badana = grupa_3

alpha = 0.05              # poziom istotności
\end{lstlisting}

\begin{lstlisting}
# test normalności Kolmogorova-Smirnova
stat_ks, p_ks = stats.kstest(grupa_badana, 'norm', 
    args=(grupa_badana.mean(), grupa_badana.std()))
test_norm = 'Kolmogorova-Smirnova'
print(f"\nStatystyka testu Kolmogorova-Smirnova: {stat_ks}")
print(f"p-wartość: {p_ks}")
if (p_ks < alpha):
    print(f"Odrzucamy hipotezę 
            o normalności rozkładu (poziom istotności: {alpha}).")
else:
    print(f"Brak podstaw do odrzucenia 
            hipotezy o normalności (poziom istotności: {alpha}).")

# test normalności Andersona-Darlinga
stat_anderson, krytyczne_wartosci,
    poziomy_istotnosci = stats.anderson(grupa_badana, dist='norm')
test_norm = 'Andersona-Darlinga'
print(f"\nStatystyka testu Andersona-Darlinga: {stat_anderson}")
print(f"Krytyczne wartości: {krytyczne_wartosci}")
print(f"Poziomy istotności: {poziomy_istotnosci}")
wartosc_krytyczna = krytyczne_wartosci[2]
if (stat_anderson > wartosc_krytyczna):
    print(f"Odrzucamy hipotezę 
            o normalności rozkładu (poziom istotności: {alpha}).")
else:
    print(f"Brak podstaw do odrzucenia 
            hipotezy o normalności (poziom istotności: {alpha}).")

# test normalności Shapiro-Wilka
stat_shapiro, p_shapiro = stats.shapiro(grupa_badana)
test_norm = 'Shapiro-Wilka'
print(f"\nStatystyka testu Shapiro-Wilka: {stat_shapiro}")
print(f"p-wartość: {p_shapiro}")
if (p_shapiro < alpha):
    print(f"Odrzucamy hipotezę 
            o normalności rozkładu (poziom istotności: {alpha}).")
else:
    print(f"Brak podstaw do odrzucenia 
            hipotezy o normalności (poziom istotności: {alpha}).")

# test normalności Lillieforsa
statystyka, p_l = lilliefors(grupa_badana)
test_norm = 'Lillieforsa'
print(f"\nStatystyka testu Lillieforsa:", {statystyka})
print(f"p-wartość:", {p_l})
if (p_l < alpha):
    print(f"Odrzucamy hipotezę 
            o normalności rozkładu (poziom istotności: {alpha}).")
else:
    print(f"Brak podstaw do odrzucenia 
            hipotezy o normalności (poziom istotności: {alpha}).")
\end{lstlisting}


Uzupełnieniem analizy statystycznej były wykresy histogramów z nałożonym rozkładem normalnym oraz wykresy Q-Q plot, które pozwalają na wizualną ocenę zgodności rozkładu empirycznego z teoretycznym rozkładem normalnym.
\newpage
Poniżej znajduje się frament kodu generujący wykresy histogramów i wykresy Q-Q.
\begin{lstlisting}
import pandas as pd
from scipy import  stats
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import skew, kurtosis

fig, axes = plt.subplots(2,2, figsize=(10, 6))
grupy = [dane[zmienna], grupa_1, grupa_2, grupa_3]
titles = [zmienna, 'grupa_1', 'grupa_2', 'grupa_3']
axes = axes.flatten()
xmin = dane[zmienna].min()
xmax = dane[zmienna].max()
bins = np.linspace(xmin, xmax, 11)
x_range = np.linspace(xmin, xmax, 100)

# histogramy
for ax, grupa, title in zip(axes, grupy, titles):
    s = skew(grupa)             # skośność
    k = kurtosis(grupa)         # kurtoza
    ax.hist(grupa, bins=bins, density=True, color='c', alpha=0.7, edgecolor='black', label="Dane", align='mid')
    mean = np.mean(grupa)       # średnia
    std_dev = np.std(grupa)     # odchylenie standardowe

    # nałożenie teoretycznego rozkładu normalnego
    xmin, xmax = ax.get_xlim()
    x = np.linspace(xmin, xmax, 100)
    p = stats.norm.pdf(x, mean, std_dev)
    ax.plot(x, p, 'r', linewidth=2, 
    label=f'Rozkład normalny\n$\mu={mean:.2f}$,
            $\sigma={std_dev:.2f}$')
    ax.set_xlim(xmin, xmax)
    ax.set_title(f'{title}\nSkosność: {s:.2f}, Kurtoza: {k:.2f}')
    ax.legend()
    
plt.tight_layout()
plt.show()

# wykresy Q-Q
fig, axes = plt.subplots(2,2, figsize=(10, 6))
grupy = [dane[zmienna], grupa_1, grupa_2, grupa_3]
titles = [zmienna, 'Grupa 1', 'Grupa 2', 'Grupa 3']
axes = axes.flatten()

for ax, grupa, title in zip(axes, grupy, titles):
    stats.probplot(grupa, dist="norm", plot=ax)
    ax.set_xlabel("Kwantyle teoretyczne")
    ax.set_ylabel("Kwantyle empiryczne")
    ax.set_title(f'Q-Q plot: {title}')

plt.tight_layout()
plt.show()
\end{lstlisting}



\newpage

\subsubsection*{Interpretacja skośności}
Skośność jest miarą asymetrii rozkładu danych względem średniej. Wartość skośności mówi, w którą stronę (i jak bardzo) dane są rozciągnięte:
\begin{itemize}
    \item skośność = 0 - rozkład jest symetryczny, jak rozkład normalny,
    \item skośność > 0 (dodatnia) - rozkład jest skośny w prawo (długi ogon po prawej stronie, większość wartości skupiona po lewej stronie),
    \item skośność < 0 (ujemna) - rozkład jest skośny w lewo (długi ogon po lewej stronie, większość wartości po prawej stronie).
\end{itemize}
Im większa bezwzględna wartość skośności, tym większa asymetria rozkładu.

\subsubsection*{Interpretacja kurtozy}
Kurtoza jest miarą „spiczastości” rozkładu, czyli tego, jak bardzo dane są skoncentrowane wokół średniej oraz jak ciężkie są ogony rozkładu.

W języku Python przy użyciu biblioteki \texttt{scipy.stats}, funkcja \texttt{kurtosis()} domyślnie zwraca kurtozę zredukowaną. Oznacza to, że:
\begin{itemize}
    \item dla rozkładu normalnego wartość kurtozy zwrócona przez kurtosis() będzie równa 0,
    \item wartości dodatnie oznaczają rozkład bardziej spiczasty (leptokurtyczny),
    \item wartości ujemne wskazują na rozkład bardziej płaski (platokurtyczny).
\end{itemize}

\subsubsection*{Interpretacja histogramu z nałożonym rozkładem normalnym}
Histogram to jedno z podstawowych narzędzi służących do wizualizacji rozkładu danych. Przedstawia on częstość występowania wartości w określonych przedziałach. Aby ocenić, czy dane mają rozkład zbliżony do normalnego, na histogram nakłada się teoretyczną krzywą rozkładu normalnego (tzw. krzywą Gaussa), obliczoną na podstawie średniej i odchylenia standardowego badanej próby.

Jeżeli kształt histogramu pokrywa się z nałożoną krzywą rozkładu normalnego (tj. jest symetryczny, przypomina dzwon, ma jedno maksimum w środku), można przypuszczać, że dane są w przybliżeniu normalnie rozłożone. W takim przypadku wartości empiryczne są zgodne z rozkładem teoretycznym.


\subsubsection*{Interpretacja wykresu Q-Q}
Wykres Q-Q (quantile-quantile) jest narzędziem służącym do graficznej oceny zgodności rozkładu empirycznego z teoretycznym rozkładem prawdopodobieństwa (u nas normalnym). Jego podstawowym celem jest sprawdzenie, czy dane pochodzą z określonego rozkładu.

Na wykresie Q-Q kwantyle empiryczne (czyli uporządkowane dane z próby) przedstawiane są na osi pionowej, natomiast kwantyle teoretyczne danego rozkładu normalnego - na osi poziomej. Punkty wykresu Q-Q reprezentują zatem pary: (kwantyl teoretyczny, kwantyl empiryczny).

 Jeśli dane rzeczywiście pochodzą z rozkładu normalnego, punkty na wykresie będą układać się wzdłuż linii prostej o kącie 45 stopni, co wskazuje na zgodność rozkładów.
 
 W dalszej części przedstawiono wyniki poszczególnych testów normalności oraz wykresy histogramów z nałożonym rozkładem normalnym i wykresów Q-Q dla każdej zmiennej.
 
\newpage
\begin{center}
    \textbf{Spotkania towarzyskie}
\end{center}
\\
Test dla grupy 1:
\begin{lstlisting}
Statystyka testu Kolmogorova-Smirnova: 0.18022601990338516
p-wartość: 2.829531771109838e-05 
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Andersona-Darlinga: 8.717745278328806
Krytyczne wartości: [0.563 0.641 0.769 0.898 1.068]
Poziomy istotności: [15.  10.   5.   2.5  1. ] 
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Shapiro-Wilka: 0.8395564560356243
p-wartość: 2.4148438345381474e-12
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Lillieforsa: 0.18022601990338516
p-wartość: 0.0009999999999998899
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).
\end{lstlisting}
\\
Test dla grupy 2:
\begin{lstlisting}
Statystyka testu Kolmogorova-Smirnova: 0.16107345314392374
p-wartość: 6.219041351851618e-05 
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Andersona-Darlinga: 5.31114108491397
Krytyczne wartości: [0.565 0.643 0.772 0.9   1.071]
Poziomy istotności: [15.  10.   5.   2.5  1. ]
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Shapiro-Wilka: 0.9084105709682497
p-wartość: 1.1066902254935294e-09
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Lillieforsa: 0.16107345314392374
p-wartość: 0.0009999999999998899
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).
\end{lstlisting}
\\
Test dla grupy 3:
\begin{lstlisting}
Statystyka testu Kolmogorova-Smirnova: 0.15021725868617353
p-wartość: 0.0015437145862611792 
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Andersona-Darlinga: 4.640400778899561
Krytyczne wartości: [0.562 0.64  0.768 0.896 1.066]
Poziomy istotności: [15.  10.   5.   2.5  1. ]
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Shapiro-Wilka: 0.899837206155958
p-wartość: 7.790568374917564e-09
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Lillieforsa: 0.15021725868617353
p-wartość: 0.0009999999999998899
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).
\end{lstlisting}

\begin{figure}[H]
    \centering
    \includegraphics[height=9.5cm]{spot_tow.pdf}
    \caption{Histogramy dla zmiennej \texttt{spot\_tow}}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[height=9.5cm]{qq-spot_tow.pdf}
    \caption{Wykresy Q-Q dla zmiennej \texttt{spot\_tow}}
\end{figure}

\subsubsection*{Wnioski dla zmiennej \texttt{spot\_tow}:}
Na podstawie wyników testów odrzucamy hipotezę o normalności rozkładu. Rozkład danych na histogramach oraz wykresy Q-Q sugerują, że dane nie pochodzą z rozkładu normalnego. Duża skośność świadczy o tym, że większość danych przyjmuje mniejsze wartości, a ujemna kurtoza o tym, że rozkład jest platokurtyczny.

\newpage
\begin{center}
    \textbf{Ekran}
\end{center}
\\
Test dla grupy 1:
\begin{lstlisting}
Statystyka testu Kolmogorova-Smirnova: 0.17453393737869705
p-wartość: 5.671259855845588e-05 
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Andersona-Darlinga: 4.704809706397043
Krytyczne wartości: [0.563 0.641 0.769 0.898 1.068]
Poziomy istotności: [15.  10.   5.   2.5  1. ]
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Shapiro-Wilka: 0.9176246237438934
p-wartość: 3.527660586464048e-08
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Lillieforsa: 0.17453393737869705
p-wartość: 0.0009999999999998899
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).
\end{lstlisting}
\\
Test dla grupy 2:
\begin{lstlisting}
Statystyka testu Kolmogorova-Smirnova: 0.17937640402884952
p-wartość: 5.1189936938679076e-06
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Andersona-Darlinga: 5.5727258602881875
Krytyczne wartości: [0.565 0.643 0.772 0.9   1.071]
Poziomy istotności: [15.  10.   5.   2.5  1. ]
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Shapiro-Wilka: 0.9219676902093393
p-wartość: 9.941244261778745e-09
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Lillieforsa: 0.17937640402884952
p-wartość: 0.0009999999999998899
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).
\end{lstlisting}
\\
Test dla grupy 3:
\begin{lstlisting}
Statystyka testu Kolmogorova-Smirnova: 0.18020081911106572
p-wartość: 6.644089863511102e-05
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Andersona-Darlinga: 5.501824046896388
Krytyczne wartości: [0.562 0.64  0.768 0.896 1.066]
Poziomy istotności: [15.  10.   5.   2.5  1. ] 
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Shapiro-Wilka: 0.8942976245927194
p-wartość: 3.794870993654891e-09
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Lillieforsa: 0.18020081911106572
p-wartość: 0.0009999999999998899
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).
\end{lstlisting}
\\
\begin{figure}[H]
    \centering
    \includegraphics[height=9.5cm]{ekran.pdf}
    \caption{Histogramy dla zmiennej \texttt{ekran}}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[height=9.5cm]{qq-ekran.pdf}
    \caption{Wykresy Q-Q dla zmiennej \texttt{ekran}}
\end{figure}

\subsubsection*{Wnioski dla zmiennej \texttt{ekran}:}
Na podstawie wyników testów odrzucamy hipotezę o normalności rozkładu. Rozkład danych na histogramach oraz wykresy Q-Q sugerują, że dane nie pochodzą z rozkładu normalnego. Duża skośność świadczy o tym, że większość danych przyjmuje mniejsze wartości, a ujemna kurtoza o tym, że rozkład jest platokurtyczny.

\newpage
\begin{center}
    \textbf{Sen}
\end{center}
\\
Test dla grupy 1:
\begin{lstlisting}
Statystyka testu Kolmogorova-Smirnova: 0.20634119396284192
p-wartość: 8.680303017930261e-07
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Andersona-Darlinga: 6.285509955906377
Krytyczne wartości: [0.563 0.641 0.769 0.898 1.068]
Poziomy istotności: [15.  10.   5.   2.5  1. ]
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Shapiro-Wilka: 0.8797281556550346
p-wartość: 2.011965488651232e-10
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Lillieforsa: 0.20634119396284192
p-wartość: 0.0009999999999998899
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).
\end{lstlisting}
\\
Test dla grupy 2:
\begin{lstlisting}
Statystyka testu Kolmogorova-Smirnova: 0.2574984721638078
p-wartość: 5.238779225880494e-12
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Andersona-Darlinga: 9.948194571577346
Krytyczne wartości: [0.565 0.643 0.772 0.9   1.071]
Poziomy istotności: [15.  10.   5.   2.5  1. ]
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Shapiro-Wilka: 0.889107169768764
p-wartość: 6.768362282733618e-11
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Lillieforsa: 0.2574984721638078
p-wartość: 0.0009999999999998899
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).
\end{lstlisting}
\\
Test dla grupy 3:
\begin{lstlisting}
Statystyka testu Kolmogorova-Smirnova: 0.20756986133964084
p-wartość: 2.2564188719038024e-06
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Andersona-Darlinga: 6.9590769797243865
Krytyczne wartości: [0.562 0.64  0.768 0.896 1.066]
Poziomy istotności: [15.  10.   5.   2.5  1. ]
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Shapiro-Wilka: 0.8804749470039538
p-wartość: 6.955140701508564e-10
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Lillieforsa: 0.20756986133964084
p-wartość: 0.0009999999999998899
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).
\end{lstlisting}

\begin{figure}[H]
    \centering
    \includegraphics[height=9.5cm]{sen.pdf}
    \caption{Histogramy dla zmiennej \texttt{sen}}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[height=9.5cm]{qq-sen.pdf}
    \caption{Wykresy Q-Q dla zmiennej \texttt{sen}}
\end{figure}

\subsubsection*{Wnioski dla zmiennej \texttt{sen}:}
Na podstawie wyników testów odrzucamy hipotezę o normalności rozkładu. Rozkład danych na histogramach oraz wykresy Q-Q sugerują, że dane nie pochodzą z rozkładu normalnego. Ujemna skośność świadczy o tym, że większość danych przyjmuje większe wartości, a duża kurtoza o tym, że rozkład jest leptokurtyczny.

\newpage
\begin{center}
    \textbf{Choroby}
\end{center}
\\
Test dla grupy 1:
\begin{lstlisting}
Statystyka testu Kolmogorova-Smirnova: 0.21290176845334874
p-wartość: 3.351151269830417e-07
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Andersona-Darlinga: 7.342138481064865
Krytyczne wartości: [0.563 0.641 0.769 0.898 1.068]
Poziomy istotności: [15.  10.   5.   2.5  1. ]
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Shapiro-Wilka: 0.8225456332102277
p-wartość: 4.654067886512983e-13
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Lillieforsa: 0.21290176845334874
p-wartość: 0.0009999999999998899
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).
\end{lstlisting}
\\
Test dla grupy 2:
\begin{lstlisting}
Statystyka testu Kolmogorova-Smirnova: 0.20092923386961226
p-wartość: 1.8990798679727724e-07
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Andersona-Darlinga: 7.3577101584622255
Krytyczne wartości: [0.565 0.643 0.772 0.9   1.071]
Poziomy istotności: [15.  10.   5.   2.5  1. ]
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Shapiro-Wilka: 0.8504092513719989
p-wartość: 5.834834351688936e-13
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Lillieforsa: 0.20092923386961226
p-wartość: 0.0009999999999998899
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).
\end{lstlisting}
\\
Test dla grupy 3:
\begin{lstlisting}
Statystyka testu Kolmogorova-Smirnova: 0.19088352920161067
p-wartość: 1.8811947255944426e-05
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Andersona-Darlinga: 4.575966603303243
Krytyczne wartości: [0.562 0.64  0.768 0.896 1.066]
Poziomy istotności: [15.  10.   5.   2.5  1. ]
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Shapiro-Wilka: 0.8935047474505112
p-wartość: 3.430297492788273e-09
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Lillieforsa: 0.19088352920161067
p-wartość: 0.0009999999999998899
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).
\end{lstlisting}

\begin{figure}[H]
    \centering
    \includegraphics[height=9.5cm]{choroby.pdf}
    \caption{Histogramy dla zmiennej \texttt{choroby}}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[height=9.5cm]{qq-choroby.pdf}
    \caption{Wykresy Q-Q dla zmiennej \texttt{choroby}}
\end{figure}

\subsubsection*{Wnioski dla zmiennej \texttt{choroby}:}
Na podstawie wyników testów odrzucamy hipotezę o normalności rozkładu. Rozkład danych na histogramach oraz wykresy Q-Q sugerują, że dane nie pochodzą z rozkładu normalnego. Duża skośność świadczy o tym, że większość danych przyjmuje mniejsze wartości, a duża kurtoza o tym, że rozkład jest leptokurtyczny.

\newpage
\begin{center}
    \textbf{Szczęście}
\end{center}
\\
Test dla grupy 1:
\begin{lstlisting}
Statystyka testu Kolmogorova-Smirnova: 0.17743186206556838
p-wartość: 3.991939619936194e-05
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Andersona-Darlinga: 4.385043231048485
Krytyczne wartości: [0.563 0.641 0.769 0.898 1.068]
Poziomy istotności: [15.  10.   5.   2.5  1. ]
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Shapiro-Wilka: 0.9327118907074158
p-wartość: 4.1479583287237935e-07
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Lillieforsa: 0.17743186206556838
p-wartość: 0.0009999999999998899
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).
\end{lstlisting}
\\
Test dla grupy 2:
\begin{lstlisting}
Statystyka testu Kolmogorova-Smirnova: 0.14809615531436332
p-wartość: 0.00030956260685285143
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Andersona-Darlinga: 2.587873825496615
Krytyczne wartości: [0.565 0.643 0.772 0.9   1.071]
Poziomy istotności: [15.  10.   5.   2.5  1. ]
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Shapiro-Wilka: 0.9619318004313192
p-wartość: 3.7028375082153926e-05
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Lillieforsa: 0.14809615531436332
p-wartość: 0.0009999999999998899
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).
\end{lstlisting}
\\
Test dla grupy 3:
\begin{lstlisting}
Statystyka testu Kolmogorova-Smirnova: 0.16921755526927068
p-wartość: 0.00022500500070539342
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Andersona-Darlinga: 3.6127573774120947
Krytyczne wartości: [0.562 0.64  0.768 0.896 1.066]
Poziomy istotności: [15.  10.   5.   2.5  1. ]
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Shapiro-Wilka: 0.9326256800049104
p-wartość: 9.959980949362528e-07
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Lillieforsa: 0.16921755526927068
p-wartość: 0.0009999999999998899
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).
\end{lstlisting}
\\
\begin{figure}[H]
    \centering
    \includegraphics[height=9.5cm]{szczęście.pdf}
    \caption{Histogramy dla zmiennej \texttt{szczęście}}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[height=9.5cm]{qq-szczęście.pdf}
    \caption{Wykresy Q-Q dla zmiennej \texttt{szczęście}}
\end{figure}

\subsubsection*{Wnioski dla zmiennej \texttt{szczęście}:}
Na podstawie wyników testów odrzucamy hipotezę o normalności rozkładu. Rozkład danych na histogramach oraz wykresy Q-Q sugerują, że dane mogą pochodzić z rozkładu normalnego. Ujemna skośność świadczy o tym, że większość danych przyjmuje mniejsze wartości, a ujemna kurtoza o tym, że rozkład jest platokurtyczny.

\newpage
Wstępna analiza normalności rozkładów wszystkich pięciu zmiennych wskazuje, że zmienna \texttt{szczęście} charakteryzuje się rozkładem najbardziej zbliżonym do normalnego. W celu pogłębienia analizy podjęto próbę normalizacji tej zmiennej za pomocą różnych transformacji: logarytmicznej, pierwiastkowej, kwantylowej oraz transformacji Box-Cox:
\begin{lstlisting}
import pandas as pd
import numpy as np
from sklearn.preprocessing import QuantileTransformer
from scipy.stats import boxcox

dane['szczęście_log'] = np.log(dane['szczęście'] + 1)
dane['szczęście_sqrt'] = np.sqrt(dane['szczęście'])
dane['szczęście_qrt'] = qt.fit_transform(dane[['szczęście']])
dane['szczęście_boxcox'], best_lambda = boxcox(dane['szczęście'] + 1)
qt = QuantileTransformer(output_distribution='normal')
\end{lstlisting}
\subsection*{Transformacje zmiennych - charakterystyka teoretyczna \cite{pqstat}}

\textbf{Transformacja logarytmiczna}
Polega na przekształceniu wartości zmiennej przy użyciu logarytmu naturalnego:
$$x'=\ln(x+c),$$
gdzie $c$ to stała dodawana w celu uniknięcia logarytmu z zera.

\vspace{2mm}
\textbf{Transformacja pierwiastkowa}
Polega na przekształceniu danych poprzez wyciągnięcie pierwiastka kwadratowego:
$$x'=\sqrt{x}.$$

\vspace{2mm}
\textbf{Transformacja kwantylowa}
Polega na przeskalowaniu wartości zmiennej w taki sposób, aby jej rozkład odpowiadał rozkładowi normalnemu. Jest to transformacja nieliniowa, oparta na uporządkowaniu danych i przypisaniu im odpowiednich kwantyli:
$$x'=\Phi^{-1}(F(x)),$$
gdzie $F(x)$ to dystrybuanta empiryczna, a $\Phi^{-1}$ to funkcja odwrotna dystrybuanty rozkładu normalnego.

\vspace{2mm}
\textbf{Transformacja Box-Coxa}
To rodzina transformacji parametrycznych z jednym parametrem 
$\lambda$, który jest dobierany automatycznie, aby najlepiej przybliżyć dane do rozkładu normalnego
$$x'=\begin{cases}
\frac{x^\lambda-1}{\lambda}, & \text{dla } \lambda \neq 0 \\
\log(x), & \text{dla } \lambda = 0
\end{cases}$$
Transformacja Box-Coxa jest stosowana tylko do danych dodatnich.

\vspace{10mm}
Pomimo zastosowania powyższych transformacji, wyniki testów normalności nie uległy poprawie - nadal odrzucamy hipotezę o normalności rozkładu. Szczegółowe wyniki testów oraz wykresy zamieszczono poniżej:
\newpage
\begin{center}
    \textbf{Transformacja logarytmiczna zmiennej \texttt{szczęście}}
\end{center}
Test dla grupy 1:
\begin{lstlisting}
Statystyka testu Kolmogorova-Smirnova: 0.23325204314904613
p-wartość: 1.4366007263006063e-08
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Andersona-Darlinga: 10.025764109450506
Krytyczne wartości: [0.563 0.641 0.769 0.898 1.068]
Poziomy istotności: [15.  10.   5.   2.5  1. ]
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Shapiro-Wilka: 0.8200024223983545
p-wartość: 3.673193044955409e-13
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Lillieforsa: 0.23325204314904613
p-wartość: 0.0009999999999998899
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).
\end{lstlisting}
Test dla grupy 2:
\begin{lstlisting}
Statystyka testu Kolmogorova-Smirnova: 0.1818156087182325
p-wartość: 3.5944804087318476e-06
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Andersona-Darlinga: 5.909225748814464
Krytyczne wartości: [0.565 0.643 0.772 0.9   1.071]
Poziomy istotności: [15.  10.   5.   2.5  1. ]
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Shapiro-Wilka: 0.8846687911191148
p-wartość: 3.7266026468489474e-11
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Lillieforsa: 0.1818156087182325
p-wartość: 0.0009999999999998899
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).
\end{lstlisting}
Test dla grupy 3:
\begin{lstlisting}
Statystyka testu Kolmogorova-Smirnova: 0.20849750475562684
p-wartość: 1.994704850449309e-06
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Andersona-Darlinga: 8.550467890370697
Krytyczne wartości: [0.562 0.64  0.768 0.896 1.066]
Poziomy istotności: [15.  10.   5.   2.5  1. ]
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Shapiro-Wilka: 0.8319974841658241
p-wartość: 4.2480591108551524e-12
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Lillieforsa: 0.20849750475562684
p-wartość: 0.0009999999999998899
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).
\end{lstlisting}
\begin{figure}[H]
    \centering
    \includegraphics[height=9.5cm]{szczęście_log.pdf}
    \caption{Histogramy dla zmiennej \texttt{szczęście\_log}}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[height=9.5cm]{qq-szczęście_log.pdf}
    \caption{Wykresy Q-Q dla zmiennej \texttt{szczęście\_log}}
\end{figure}
\subsubsection*{Wnioski dla zmiennej \texttt{szczęście\_log}:}
Na podstawie wyników testów odrzucamy hipotezę o normalności rozkładu. Rozkład danych na histogramach oraz wykresy Q-Q sugerują, że dane nie pochodzą z rozkładu normalnego. Ujemna skośność świadczy o tym, że większość danych przyjmuje mniejsze wartości, a duża kurtoza o tym, że rozkład jest leptokurtyczny. Transformacja danych nie poprawiła wyników.

\newpage
\begin{center}
    \textbf{Transformacja pierwiastkowa zmiennej \texttt{szczęście}}
\end{center}
Test dla grupy 1:
\begin{lstlisting}
Statystyka testu Kolmogorova-Smirnova: 0.21077761804660133
p-wartość: 4.576059387923028e-07
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Andersona-Darlinga: 7.333977961405708
Krytyczne wartości: [0.563 0.641 0.769 0.898 1.068]
Poziomy istotności: [15.  10.   5.   2.5  1. ]
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Shapiro-Wilka: 0.8724919490858201
p-wartość: 8.493355497523283e-11
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Lillieforsa: 0.21077761804660133
p-wartość: 0.0009999999999998899
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).
\end{lstlisting}
Test dla grupy 2:
\begin{lstlisting}
Statystyka testu Kolmogorova-Smirnova: 0.17009419586502067
p-wartość: 1.879706453372998e-05
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Andersona-Darlinga: 4.036967299156004
Krytyczne wartości: [0.565 0.643 0.772 0.9   1.071]
Poziomy istotności: [15.  10.   5.   2.5  1. ]
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Shapiro-Wilka: 0.9270016690353533
p-wartość: 2.3846661111915482e-08
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Lillieforsa: 0.17009419586502067
p-wartość: 0.0009999999999998899
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).
\end{lstlisting}
Test dla grupy 3:
\begin{lstlisting}
Statystyka testu Kolmogorova-Smirnova: 0.19513280447818276
p-wartość: 1.1154205499313904e-05
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Andersona-Darlinga: 6.100904111817613
Krytyczne wartości: [0.562 0.64  0.768 0.896 1.066]
Poziomy istotności: [15.  10.   5.   2.5  1. ]
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Shapiro-Wilka: 0.8809823769534055
p-wartość: 7.385281263058607e-10
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Lillieforsa: 0.19513280447818276
p-wartość: 0.0009999999999998899
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).
\end{lstlisting}
\begin{figure}[H]
    \centering
    \includegraphics[height=9.5cm]{szczęście_sqrt.pdf}
    \caption{Histogramy dla zmiennej \texttt{szczęście\_sqrt}}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[height=9.5cm]{qq-szczęście_sqrt.pdf}
    \caption{Wykresy Q-Q dla zmiennej \texttt{szczęście\_sqrt}}
\end{figure}
\subsubsection*{Wnioski dla zmiennej \texttt{szczęście\_sqrt}:}
Na podstawie wyników testów odrzucamy hipotezę o normalności rozkładu. Rozkład danych na histogramach oraz wykresy Q-Q sugerują, że dane nie pochodzą z rozkładu normalnego. Ujemna skośność świadczy o tym, że większość danych przyjmuje mniejsze wartości, a duża kurtoza o tym, że rozkład jest leptokurtyczny. Transformacja danych nie poprawiła wyników.

\newpage
\begin{center}
    \textbf{Transformacja kwantylowa zmiennej \texttt{szczęście}}
\end{center}
Test dla grupy 1:
\begin{lstlisting}
Statystyka testu Kolmogorova-Smirnova: 0.1945882122272432
p-wartość: 4.4216966099126715e-06
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Andersona-Darlinga: 11.300187163513016
Krytyczne wartości: [0.563 0.641 0.769 0.898 1.068]
Poziomy istotności: [15.  10.   5.   2.5  1. ]
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Shapiro-Wilka: 0.7764175706329818
p-wartość: 8.771481709096798e-15
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Lillieforsa: 0.1945882122272432
p-wartość: 0.0009999999999998899
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).
\end{lstlisting}
Test dla grupy 2:
\begin{lstlisting}
Statystyka testu Kolmogorova-Smirnova: 0.1988556019649187
p-wartość: 2.6512710976767987e-07
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Andersona-Darlinga: 11.491491669308402
Krytyczne wartości: [0.565 0.643 0.772 0.9   1.071]
Poziomy istotności: [15.  10.   5.   2.5  1. ]
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Shapiro-Wilka: 0.7974901713724707
p-wartość: 2.9612917837343903e-15
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Lillieforsa: 0.1988556019649187
p-wartość: 0.0009999999999998899
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).
\end{lstlisting}
Test dla grupy 3:
\begin{lstlisting}
Statystyka testu Kolmogorova-Smirnova: 0.2244901707687652
p-wartość: 2.1768571452455345e-07
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Andersona-Darlinga: 9.06469016055567
Krytyczne wartości: [0.562 0.64  0.768 0.896 1.066]
Poziomy istotności: [15.  10.   5.   2.5  1. ]
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Shapiro-Wilka: 0.8277828333656936
p-wartość: 2.8649590726574235e-12
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Lillieforsa: 0.2244901707687652
p-wartość: 0.0009999999999998899
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).
\end{lstlisting}
\begin{figure}[H]
    \centering
    \includegraphics[height=8.7cm]{szczęście_qrt.pdf}
    \caption{Histogramy dla zmiennej \texttt{szczęście\_qrt}}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[height=9.5cm]{qq-szczęście_qrt.pdf}
    \caption{Wykresy Q-Q dla zmiennej \texttt{szczęście\_qrt}}
\end{figure}
\subsubsection*{Wnioski dla zmiennej \texttt{szczęście\_qrt}:}
Na podstawie wyników testów odrzucamy hipotezę o normalności rozkładu. Rozkład danych na histogramach oraz wykresy Q-Q sugerują, że dane nie pochodzą z rozkładu normalnego. Daleka od zera skośność świadczy o tym, że większość danych przyjmuje skrajne wartości, a duża kurtoza o tym, że rozkład jest leptokurtyczny. Transformacja danych nie poprawiła wyników.

\newpage
\begin{center}
    \textbf{Transformacja Box-Coxa zmiennej \texttt{szczęście}}
\end{center}
Test dla grupy 1:
\begin{lstlisting}
Statystyka testu Kolmogorova-Smirnova: 0.16423835444433765
p-wartość: 0.0001882438274642448
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Andersona-Darlinga: 3.0892411702167806
Krytyczne wartości: [0.563 0.641 0.769 0.898 1.068]
Poziomy istotności: [15.  10.   5.   2.5  1. ]
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Shapiro-Wilka: 0.9552844640383937
p-wartość: 3.220326187373239e-05
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Lillieforsa: 0.16423835444433765
p-wartość: 0.0009999999999998899
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).
\end{lstlisting}
Test dla grupy 2:
\begin{lstlisting}
Statystyka testu Kolmogorova-Smirnova: 0.12427480070853764
p-wartość: 0.004131078262880801
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Andersona-Darlinga: 2.341283190181116
Krytyczne wartości: [0.565 0.643 0.772 0.9   1.071]
Poziomy istotności: [15.  10.   5.   2.5  1. ]
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Shapiro-Wilka: 0.9671453977022427
p-wartość: 0.00014490953739533746
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Lillieforsa: 0.12427480070853764
p-wartość: 0.0009999999999998899
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).
\end{lstlisting}
Test dla grupy 3:
\begin{lstlisting}
Statystyka testu Kolmogorova-Smirnova: 0.1457496056011175
p-wartość: 0.0023472996481965483
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Andersona-Darlinga: 2.5765868538165932
Krytyczne wartości: [0.562 0.64  0.768 0.896 1.066]
Poziomy istotności: [15.  10.   5.   2.5  1. ]
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Shapiro-Wilka: 0.9516418355775726
p-wartość: 3.1554588706364266e-05
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).

Statystyka testu Lillieforsa: 0.1457496056011175
p-wartość: 0.0009999999999998899
Odrzucamy hipotezę o normalności rozkładu (poziom istotności: 0.05).
\end{lstlisting}
\begin{figure}[H]
    \centering
    \includegraphics[height=8.7cm]{szczęście_boxcox.pdf}
    \caption{Histogramy dla zmiennej \texttt{szczęście\_boxcox}}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[height=9.5cm]{qq-szczęście_boxcox.pdf}
    \caption{Wykresy Q-Q dla zmiennej \texttt{szczęście\_boxcox}}
\end{figure}
\subsubsection*{Wnioski dla zmiennej \texttt{szczęście\_boxcox}:}
Na podstawie wyników testów odrzucamy hipotezę o normalności rozkładu. Rozkład danych na histogramach oraz wykresy Q-Q sugerują, że mogą pochodzić z rozkładu normalnego. Bliska zera skośność świadczy o względnie dobrej symetrii rozkładu, a ujemna kurtoza o tym, że rozkład jest platokurtyczny. Transformacja danych nieznacznie poprawiła wyniki.

\newpage

W dalszej części analizy rozważać będziemy jedynie zmienną \texttt{szczęście}, ponieważ jej rozkład najbardziej przypomina rozkład normalny spośród analizowanych zmiennych. Niemniej jednak, z uwagi na fakt, że wszystkie przeprowadzone testy statystyczne odrzuciły hipotezę zerową o normalności rozkładu tej zmiennej, zastosujemy \textbf{analizę wariancji Welch'a} oraz \textbf{nieparametryczny test Kruskala-Wallisa}.

\vspace{4mm}
\textbf{Założenia analizy wariancji Welch'a:}
\begin{itemize}
    \item niezależność obserwacji - dane w każdej grupie powinny pochodzić z niezależnych prób,
    \item skala interwałowa lub ilorazowa - zmienna zależna powinna być mierzona co najmniej na poziomie interwałowym,
    \item brak założenia równości wariancji - w przeciwieństwie do klasycznej analizy ANOVA, test Welch’a nie wymaga homogeniczności wariancji między grupami,
    \item \underline{rozkład zbliżony do normalnego} - choć test jest odporny na odstępstwa od normalności, nadal preferowane są dane o rozkładzie symetrycznym lub zbliżonym do normalnego w każdej grupie.
\end{itemize}

\textbf{Założenia testu Kruskala-Wallisa:}
\begin{itemize}
    \item niezależność obserwacji - dane w każdej grupie powinny pochodzić z niezależnych prób,
    \item skala porządkowa lub wyższa - zmienna zależna powinna być mierzalna co najmniej na skali porządkowej (porządkowa, przedziałowa lub ilorazowa),
    \item \underline{brak założenia normalności rozkładu}.
\end{itemize}


\vspace{10mm}

\subsubsection{Test homogeniczności wariancji}
Przed przystąpieniem do analizy różnic między grupami, sprawdzimy, czy zmienna \texttt{szczęście} spełnia założenie jednorodności wariancji w obrębie porównywanych grup. Choć testy, które zostaną zastosowane w dalszej części (np. test Welch’a lub test Kruskala-Wallisa) nie wymagają równości wariancji, informacja ta pozwala lepiej dostosować metodę analizy do charakterystyki danych.

\vspace{3mm}
\\Zastosowano trzy różne testy:
\begin{enumerate}
    \item Test Levene’a - mniej wrażliwy na odchylenia od normalności, zalecany przy podejrzeniu braku normalności rozkładu,
    \item Test Bartletta - bardziej czuły, ale wymaga normalności danych,
    \item Test Flignera-Killeena - nieparametryczny test, odporny na brak normalności.
\end{enumerate}
\newpage
\subsection*{Testy homogeniczności wariancji}

W celu weryfikacji założenia o równości wariancji między grupami, zastosowano trzy testy: test Levene’a, test Bartletta oraz test Flignera-Killeena. Każdy z nich testuje hipotezę zerową o równości wariancji pomiędzy $k$ grupami:

$$
H_0: \sigma_1^2 = \sigma_2^2 = \dots = \sigma_k^2.
$$
\vspace{10mm}
\paragraph{Test Levene’a \cite{levene1960robust}}
Test Levene’a opiera się na analizie odchyleń wartości od średniej lub mediany w każdej grupie. Dla każdej obserwacji oblicza się:

$$
Z_{ij} = |X_{ij} - \overline{X}_i|
$$

gdzie 
\begin{itemize}
    \item $Z_{ij}$ - przekształcona wartość dla $i$-tej grupy i $j$-tej obserwacji,
    \item $X_{ij}$ - oryginalna wartość dla $i$-tej grupy i $j$-tej obserwacji,
    \item $\overline{X}_i$ - średnia lub mediana w grupie $i$.
\end{itemize} 
Następnie obliczana jest statystyka testowa:

$$
W = \frac{(N - k)}{(k - 1)} \cdot \frac{\sum_{i=1}^{k} n_i (\overline{Z}_{i} - \overline{Z})^2}{\sum_{i=1}^{k} \sum_{j=1}^{n_i} (Z_{ij} - \overline{Z}_{i})^2}
,$$
gdzie:
\begin{itemize}
    \item $k$ - liczba grup,
    \item $n_i$ - liczba obserwacji w $i$-tej grupie,
    \item $N = \sum_{i=1}^{k} n_i$ - łączna liczba obserwacji,
    \item $\overline{Z}_{i}$ - średnia wartość $Z_{ij}$ w $i$-tej grupie,
    \item $\overline{Z}$ - średnia wartość wszystkich $Z_{ij}$.
\end{itemize}
Statystyka $W$ ma rozkład $F$ z $k - 1$ oraz $N - k$ stopniami swobody. Test ten jest odporny na nienormalność rozkładu, zwłaszcza w wersji opartej na medianie.

\newpage
\paragraph{Test Bartletta \cite{bartlett1937properties}}
Test Bartletta oparty jest na transformacji logarytmicznej wariancji. Statystyka testowa ma postać:

$$
\chi^2 = \frac{(N - k) \cdot \ln S_p^2 - \sum_{i=1}^{k} (n_i - 1) \cdot \ln s_i^2}{1 + \frac{1}{3(k - 1)} \left( \sum_{i=1}^{k} \frac{1}{n_i - 1} - \frac{1}{N - k} \right)},
$$

gdzie 
\begin{itemize}
     \item $k$ - liczba porównywanych grup,
      \item $n_i$ - liczba obserwacji w $i$-tej grupie,
      \item $N = \sum_{i=1}^{k} n_i$ - łączna liczba obserwacji,
    \item $S_p^2$ - uśredniona wariancja ważona:

$$
S_p^2 = \frac{1}{N - k} \sum_{i=1}^{k} (n_i - 1) s_i^2
$$
\end{itemize}

Statystyka testowa ma przybliżony rozkład chi-kwadrat z $k - 1$ stopniami swobody. Ze względu na wysoką wrażliwość na odstępstwa od normalności, test Bartletta stosowany jest głównie w przypadku danych spełniających to założenie.

\newpage
\paragraph{Test Flignera-Killeena \cite{fligner1976distribution}}

Test Flignera-Killeena to nieparametryczny test jednorodności wariancji, charakteryzujący się wysoką odpornością na odstępstwa od normalności rozkładów. Procedura opiera się na rangach bezwzględnych odchyleń od mediany w obrębie grup oraz transformacji rang z wykorzystaniem odwrotności dystrybuanty rozkładu normalnego.

Dla każdej obserwacji $X_{ij}$ oblicza się odchylenie względem mediany w grupie $i$:

$$
D_{ij} = |X_{ij} - \text{mediana}(X_i)|
$$

Następnie wartości $D_{ij}$ są rangowane globalnie (w całym zbiorze danych), a otrzymanym rangom $R_{ij}$ przypisuje się transformacje:

$$
Z_{ij} = \Phi^{-1} \left( \frac{R_{ij}}{N + 1} \right)
$$

gdzie $\Phi^{-1}$ to odwrotność dystrybuanty standardowego rozkładu normalnego.

Dla każdej grupy obliczane są wartości:

$$
A_{ij} = |Z_{ij} - \tilde{Z}_i|
$$

gdzie $\tilde{Z}_i$ to mediana wartości $Z_{ij}$ w grupie $i$. Na podstawie wartości $A_{ij}$ oblicza się statystykę testową na wzór analizy wariancji:

$$
F = \frac{\sum_{i=1}^{k} n_i (\overline{A}_i - \overline{A})^2 / (k - 1)}{\sum_{i=1}^{k} \sum_{j=1}^{n_i} (A_{ij} - \overline{A}_i)^2 / (N - k)}
$$

Statystyka $F$ ma w przybliżeniu rozkład Fishera z $k - 1$ oraz $N - k$ stopniami swobody. W niektórych wersjach procedury stosuje się również statystykę $\chi^2$, uzyskaną z analizy wariancji wartości rangowanych lub ich transformacji.

gdzie:

\begin{itemize}
  \item $k$ - liczba porównywanych grup,
  \item $n_i$ - liczba obserwacji w $i$-tej grupie,
  \item $N = \sum_{i=1}^{k} n_i$ - łączna liczba obserwacji,
  \item $R_{ij}$ - ranga wartości $D_{ij} = |X_{ij} - \text{mediana}(X_i)|$ w całym zbiorze,
  \item $\Phi^{-1}$ - odwrotność dystrybuanty standardowego rozkładu normalnego,
  \item $Z_{ij}$ - przekształcone rangi,
  \item $A_{ij}$ - wartości bezwzględnych odchyleń od mediany przekształconych rang,
  \item $\overline{A}_i$ - średnia z $A_{ij}$ w grupie $i$, 
  \item $\overline{A}$ - średnia globalna.
\end{itemize}




\newpage
Poniżej przedstawiono fragment kodu w języku Python przeprowadzający testy jednorodności wariancji pomiędzy grupami dla zmiennej \texttt{szczęście}.
\begin{lstlisting}
from scipy import  stats

alpha = 0.05

# test Levene'a:
stat_levene, p_levene = stats.levene(grupa_1, grupa_2, grupa_3)
test_wariancji = 'Levenea'
print(f"\nStatystyka testu {test_wariancji}: {stat_levene}")
print(f"p-wartość: {p_levene}")
if p_levene < alpha:
    print("Odrzucamy hipotezę o równości wariancji.")
else:
    print("Brak podstaw do odrzucenia hipotezy o równości wariancji.")

# test Bartletta:
stat_bartlett, p_bartlett = stats.bartlett(grupa_1, grupa_2, grupa_3)
test_wariancji = 'Bartletta'
print(f"\nStatystyka testu {test_wariancji}: {stat_bartlett}")
print(f"p-wartość: {p_bartlett}")
if p_bartlett < alpha:
    print("Odrzucamy hipotezę o równości wariancji.")
else:
    print("Brak podstaw do odrzucenia hipotezy o równości wariancji.")

# test Flignera-Killeena:
stat_fligner, p_fligner = stats.fligner(grupa_1, grupa_2, grupa_3)
test_wariancji = 'Flignera-Killeena'
print(f"\nStatystyka testu {test_wariancji}: {stat_fligner}")
print(f"p-wartość: {p_fligner}")
if p_fligner < alpha:
    print("Odrzucamy hipotezę o równości wariancji.")
else:
    print("Brak podstaw do odrzucenia hipotezy o równości wariancji.")
\end{lstlisting}
Wyniki testów:
\begin{lstlisting}
Statystyka testu Levenea: 1.070637909187288
p-wartość: 0.34354562195174554
Brak podstaw do odrzucenia hipotezy o równości wariancji.

Statystyka testu Bartletta: 5.324178562274529
p-wartość: 0.06980223280345424
Brak podstaw do odrzucenia hipotezy o równości wariancji.

Statystyka testu Flignera-Killeena: 2.0652441874080374
p-wartość: 0.3560720810720071
Brak podstaw do odrzucenia hipotezy o równości wariancji.
\end{lstlisting}
Z uwagi na niespełnienie założenia normalności rozkładu danych, analizie poddano jedynie wyniki testów Levene’a oraz Flignera-Killeena, które nie wymagają tego założenia. Na podstawie uzyskanych wyników można stwierdzić, że wariancje pomiędzy badanymi grupami są homogeniczne.

\newpage
\subsubsection{Analiza wariancji}
Tym samym dotarliśmy do kluczewego momentu naszej analizy, czyli do analizy wariancji. [...]

\newpage
\subsection{Badanie zależności między [...]}
\subsubsection{Opis danych}
\subsubsection{Analiza statystyczna zebranych danych}


\newpage


\section{Bibliografia}

Pozycje \cite{gitkkozlowski}, \cite{pandas}, \cite{statsmodels} służyły jako pomoc przy pisaniu kodów w LaTeX i Python.
\bibliographystyle{plain}
\bibliography{bibliografia}



\end{document}
